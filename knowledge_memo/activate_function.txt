・活性化関数：
    - 入力信号の総和がどのように活性化するかを決定する役割
    - 次の層に渡す値を整えるような役割


非線形関数
・ステップ関数：
    - 単純パーセプトロン（単層のネットワーク）で使われる関数
    - コード（正解か不正解か）
    def step_function(x):
    if x>0:
        return 1
    else:
        return 0

・シグモイド関数：
    - 入力値が大きいほど１,小さいほど０
    - コード
    def sigmoid(x):
        return 1 / (1+np.exp(-x))

・ReLU関数：
    - 入力した値が0以下のとき0になり、1より大きいとき入力をそのまま出力
    - コード
    def relu(x):
        return np.maximum(0, x)

・恒等関数：
    - 出力層で使われる関数
    - 入力した値と同じ値を常にそのまま返す関数
    - 回帰問題で使用
    - コード
    def koutou(a):
        return a

・ソフトマックス関数：
    - 出力層で使われる関数
    - 一般的に分類問題で使用
    - 和が100%の正規分布を作る
    - コード
    def softmax(a):
        exp_a = np.exp(a)           #np.exp(x) はeのx乗を返す
        sum_exp_a = np.sum(exp_a)
        y = exp_a / sum_exp_a
        return y

・機械学習の補足知識その１
多層パーセプトロンでは線形関数を使用しない
h(x)=cx と仮定して３層重ねると
y=h(h(h(x)) => y=c*c*c*x(y=ax) => 多層の意味がない

・機械学習の補足知識その２
パーセプトロンによるクラスタリングでは決定領域が直線にならない場合は学習できない
（何回学習して重みが一定値に収束しない）


・参考URL：
    - 活性化関数
    https://qiita.com/namitop/items/d3d5091c7d0ab669195f
    - Pythonと機械学習
    http://darden.hatenablog.com/entry/2016/08/10/205717

　